{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7A3s57blp7x",
        "outputId": "9bc8a642-71ee-4c00-80b0-be54440627fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "A journey of a thousand miles begins with a single step.\n",
        "Generative AI is transforming how we interact with technology.\n",
        "Neural networks are powerful tools for pattern recognition.\n",
        "Deep learning models require vast amounts of data.\n",
        "\"\"\"\n",
        "\n",
        "# Convert text to lowercase for consistency\n",
        "text = text.lower()\n",
        "\n",
        "# Create a sorted list of unique characters in the text\n",
        "vocab = sorted(list(set(text)))\n",
        "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
        "int_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "\n",
        "# Prepare training sequences\n",
        "seq_length = 50 # Length of input sequences\n",
        "data_X = [] # Input sequences\n",
        "data_y = [] # Output characters (next character in sequence)\n",
        "\n",
        "# Create sequences of characters and their corresponding next character\n",
        "for i in range(0, len(text) - seq_length):\n",
        "    seq_in = text[i:i + seq_length]\n",
        "    seq_out = text[i + seq_length]\n",
        "    data_X.append([char_to_int[char] for char in seq_in])\n",
        "    data_y.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(data_X)\n",
        "print(f\"Total patterns: {n_patterns}\")\n",
        "\n",
        "# Reshape X to be [samples, time steps] for Embedding layer input\n",
        "X = np.reshape(data_X, (n_patterns, seq_length))\n",
        "\n",
        "# One-hot encode the output variable (y) - not needed for sparse_categorical_crossentropy\n",
        "y = np.array(data_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zemGLSeYmhi1",
        "outputId": "aed86af3-7e29-4831-92e2-d35716a41840"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 29\n",
            "Vocabulary: ['\\n', ' ', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Total patterns: 227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # Embedding layer maps integer indices to dense vectors\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=seq_length, input_shape=(seq_length,)),\n",
        "    # LSTM layer to learn sequential dependencies\n",
        "    LSTM(256, return_sequences=True), # return_sequences=True for stacking LSTMs\n",
        "    Dropout(0.2), # Dropout for regularization\n",
        "    LSTM(256), # Last LSTM layer does not return sequences\n",
        "    Dropout(0.2),\n",
        "    # Dense layer for output, with softmax activation for probability distribution over vocabulary\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Use Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.005)\n",
        "# Use sparse_categorical_crossentropy because y is integer-encoded, not one-hot\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "kfukJoTcmqff",
        "outputId": "6976b1d0-5dd0-46b0-c38e-75a5e1e62ad5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m7,424\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │         \u001b[38;5;34m7,453\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,453</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,065,501\u001b[0m (4.06 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,065,501</span> (4.06 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,065,501\u001b[0m (4.06 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,065,501</span> (4.06 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01508cc8",
        "outputId": "792938c5-2df2-46cb-e2ec-d7a4351fe871"
      },
      "source": [
        "print(\"\\nStarting model training...\")\n",
        "# Train the model\n",
        "# Using a small number of epochs for demonstration.\n",
        "# For better results, increase epochs and use callbacks like ModelCheckpoint.\n",
        "history = model.fit(X, y, epochs=20, batch_size=128, verbose=1)\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 771ms/step - loss: 3.3485\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 779ms/step - loss: 3.5544\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 740ms/step - loss: 3.0617\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 799ms/step - loss: 2.9766\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - loss: 2.9441\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 789ms/step - loss: 2.9088\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 780ms/step - loss: 2.9585\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 803ms/step - loss: 2.9534\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - loss: 2.9419\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793ms/step - loss: 3.1306\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 773ms/step - loss: 2.8598\n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 800ms/step - loss: 2.7912\n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step - loss: 2.7237\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - loss: 2.6956\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 797ms/step - loss: 2.6161\n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 766ms/step - loss: 2.5495\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 803ms/step - loss: 2.4550\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - loss: 2.2849\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941ms/step - loss: 2.2666\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 796ms/step - loss: 2.1349\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, num_chars_to_generate=200, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text using the trained LSTM model.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The trained Keras model.\n",
        "        seed_text (str): The initial text to start generation from.\n",
        "        num_chars_to_generate (int): The number of characters to generate.\n",
        "        temperature (float): Controls the randomness of predictions.\n",
        "                             Lower temp (e.g., 0.2) makes text more predictable.\n",
        "                             Higher temp (e.g., 1.0) makes text more random/creative.\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text.lower()\n",
        "    pattern = [char_to_int[char] for char in generated_text[-seq_length:]] # Use last part of seed if too long\n",
        "\n",
        "    print(f\"\\nGenerating text with seed: \\\"{seed_text}\\\"\")\n",
        "    print(f\"Temperature: {temperature}\")\n",
        "\n",
        "    for i in range(num_chars_to_generate):\n",
        "        # Reshape input for the model: (1, seq_length, 1)\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        # Predict probabilities for the next character\n",
        "        prediction_probabilities = model.predict(x, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature to probabilities\n",
        "        prediction_probabilities = np.log(prediction_probabilities) / temperature\n",
        "        exp_preds = np.exp(prediction_probabilities)\n",
        "        prediction_probabilities = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Sample the next character based on probabilities\n",
        "        next_char_int = np.random.choice(len(vocab), p=prediction_probabilities)\n",
        "        next_char = int_to_char[next_char_int]\n",
        "\n",
        "        generated_text += next_char\n",
        "        # Update the pattern for the next prediction\n",
        "        pattern.append(next_char_int)\n",
        "        pattern = pattern[1:len(pattern)] # Keep the pattern length consistent\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "7nZ02VkhnbDD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Demonstrating Text Generation\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Example 1: Generate text with a common seed\n",
        "seed1 = \"the quick brown fox\"\n",
        "generated_output1 = generate_text(model, seed1, num_chars_to_generate=150, temperature=0.5)\n",
        "print(f\"\\nGenerated Text 1:\\n{generated_output1}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Example 2: Generate text with a different seed and higher temperature\n",
        "seed2 = \"neural networks\"\n",
        "generated_output2 = generate_text(model, seed2, num_chars_to_generate=150, temperature=1.0)\n",
        "print(f\"\\nGenerated Text 2:\\n{generated_output2}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Example 3: Generate text with a very short seed\n",
        "seed3 = \"deep learning\"\n",
        "generated_output3 = generate_text(model, seed3, num_chars_to_generate=150, temperature=0.7)\n",
        "print(f\"\\nGenerated Text 3:\\n{generated_output3}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h68DAEA_njI1",
        "outputId": "4e94642f-eb8e-4cab-c7c4-9647794a95d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Demonstrating Text Generation\n",
            "==================================================\n",
            "\n",
            "\n",
            "Generating text with seed: \"the quick brown fox\"\n",
            "Temperature: 0.5\n",
            "\n",
            "Generated Text 1:\n",
            "the quick brown foxa touss aratrrnoa iup rns rrhrrins mitss iii ina  iss itg bhes etniiia ins twwis ein awtaas ts ens iis iws iis iint tooss rraar rrr rrrrt  aais ttaln \n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Generating text with seed: \"neural networks\"\n",
            "Temperature: 1.0\n",
            "\n",
            "Generated Text 2:\n",
            "neural networks r atu ufmnot nohfi atiiss rosaf  rucs vgtcooss  rnn gbisoh.epn\n",
            "elsf atitrts mrfv inf attwat. ana\n",
            "netenis ags ipo.eipuaiavrh a wins tsq o.rsrrv nttouo\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Generating text with seed: \"deep learning\"\n",
            "Temperature: 0.7\n",
            "\n",
            "Generated Text 3:\n",
            "deep learning twwnen g itls eeeraan ttmtls nerrwteg eitnhhoowgl geiwss zi.eure ant smonls  wwrtry etaaonn wvnhs rmwnusg ftid..eeraany oaas eavt s onr s frowrns ore\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}